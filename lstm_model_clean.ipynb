{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noah Venethongkham, 219660117\n",
    "# Ashley Thor, 219334909\n",
    "# Lucas Saechao, 218794239\n",
    "# CSC 180 - Intelligent Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucassaechao/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucassaechao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "# scikit learn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import column_or_1d\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "import skimage.transform\n",
    "\n",
    "# natural language toolkit\n",
    "# run pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# tensorflow and keras\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, Activation, Flatten, Dropout, Conv1D, Conv2D, GlobalMaxPooling1D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# run pip install np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# python libraries\n",
    "from collections.abc import Sequence\n",
    "import requests\n",
    "import pathlib\n",
    "import shutil\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "\n",
    "# Plots a confusion matrix for the model\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Plot an ROC curve\n",
    "def plot_roc(pred, y):\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred)\n",
    "    roc_area_under_curve = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = $0.2f)' % roc_area_under_curve)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show\n",
    " \n",
    "def text_to_word_list(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    # clean text by regex\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=><]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\>\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"\\'\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", \"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"covid19\", \"covid\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub('_', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = text.split()\n",
    "    words_clean = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n",
    "    return \" \".join(words_clean)\n",
    "    \n",
    "def remove_stop_manual(data):\n",
    "    stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
    "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
    "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
    "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
    "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
    "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
    "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "    data = data.apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    return data\n",
    "    \n",
    "def read_glove_vector(glove_vec):\n",
    "    with open(glove_vec, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            w_line = line.split()\n",
    "            current_word = w_line[0]\n",
    "            word_to_vec_map[current_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map\n",
    "    \n",
    "def lstm_model(input_shape):\n",
    "    x_indices = Input(input_shape)\n",
    "    embeddings = embedding_layer(x_indices)\n",
    "    x = LSTM(128, return_sequences=True)(embeddings)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LSTM(128)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=x_indices, outputs=x)\n",
    "    return model\n",
    "\n",
    "def conv_model(input_shape):\n",
    "    x_indices = Input(input_shape)\n",
    "    embeddings = embedding_layer(x_indices)\n",
    "    x = Conv1D(512, 3, activation='relu')(embeddings)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Conv1D(256, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Conv1D(256, 3, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=x_indices, outputs=x)\n",
    "    return model\n",
    "    \n",
    "def predict_sentiments(data, corpus):\n",
    "    data['sentiment score'] = 0\n",
    "    corpus = pad_sequences(corpus, maxlen=max_len, padding='post')\n",
    "    pred = model.predict(corpus)\n",
    "    data['sentiment score'] = pred\n",
    "    pred_sentiment = np.array(list(map(lambda x: 'positive' if x > 0.5 else 'negative', pred)))\n",
    "    data['predicted sentiment'] = 0\n",
    "    data['predicted sentiment'] = pred_sentiment\n",
    "    return data\n",
    "    \n",
    "# Beep if on a windows machine\n",
    "if os.name == 'nt':\n",
    "    def ding():\n",
    "        winsound.Beep(2000, 300)\n",
    "        winsound.Beep(2000, 300)\n",
    "        winsound.Beep(2000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  sentiment\n",
      "0     0 125 0 82 mg aluminium vaccines applejuice 10...          1\n",
      "1     1 autism not just genetic behavioural diagnost...          1\n",
      "2     well many many scientists believe vaccines saf...         -1\n",
      "3     1 freedom of 2 looking freedom information act...         -1\n",
      "4     2 never said andrew wakefield jailed lose medi...         -1\n",
      "...                                                 ...        ...\n",
      "1318                      youre idiot gave exact answer         -1\n",
      "1319                youre right also using food example         -1\n",
      "1320  zerg admit vaccine contains thimerosal contain...         -1\n",
      "1321  zero links autism charts argument debunked due...          1\n",
      "1322               zika virus problem vaccine obviously         -1\n",
      "\n",
      "[1323 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create pandas dataframe output file\n",
    "df_reddit = pd.read_csv('reddit_vm_clean.csv', encoding=\"utf-8\")\n",
    "df_reddit = df_reddit[['text', 'sentiment']].fillna('')\n",
    "\n",
    "print(df_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 125 0 82 mg aluminium vaccines applejuice 10...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 autism not just genetic behavioural diagnost...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well many many scientists believe vaccines saf...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 freedom of 2 looking freedom information act...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 never said andrew wakefield jailed lose medi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>youre idiot gave exact answer</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>youre right also using food example</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>zerg admit vaccine contains thimerosal contain...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>zero links autism charts argument debunked due...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>zika virus problem vaccine obviously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1323 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment\n",
       "0     0 125 0 82 mg aluminium vaccines applejuice 10...  positive\n",
       "1     1 autism not just genetic behavioural diagnost...  positive\n",
       "2     well many many scientists believe vaccines saf...  negative\n",
       "3     1 freedom of 2 looking freedom information act...  negative\n",
       "4     2 never said andrew wakefield jailed lose medi...  negative\n",
       "...                                                 ...       ...\n",
       "1318                      youre idiot gave exact answer  negative\n",
       "1319                youre right also using food example  negative\n",
       "1320  zerg admit vaccine contains thimerosal contain...  negative\n",
       "1321  zero links autism charts argument debunked due...  positive\n",
       "1322               zika virus problem vaccine obviously  negative\n",
       "\n",
       "[1323 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = df_reddit.sentiment\n",
    "df_sentiment.replace(-1, 'negative', inplace=True)\n",
    "df_sentiment.replace(1, 'positive', inplace=True)\n",
    "df_reddit.sentiment = df_sentiment\n",
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = df_reddit.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>next time try not come things place bad faith ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use youtube video try prove polio vaccine bad ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>irrefutable prufe</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that bit crazy opinion would making illegal no...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sure am know mercury vaccines doctors told mil...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  next time try not come things place bad faith ...  negative\n",
       "1  use youtube video try prove polio vaccine bad ...  negative\n",
       "2                                  irrefutable prufe  negative\n",
       "3  that bit crazy opinion would making illegal no...  negative\n",
       "4  sure am know mercury vaccines doctors told mil...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.text = df_corpus.text.apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_corpus.text\n",
    "sentiments = df_corpus.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for i in range(len(texts)):\n",
    "    corpus_list.append(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, sentiments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(corpus_list, y, test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926\n",
      "397\n",
      "926\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_loc = os.path.join(\n",
    "    os.path.expanduser('~'), '.keras/datasets/glove.6B.300d.txt'\n",
    ")\n",
    "word_to_vec_map = read_glove_vector(glove_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6256\n",
      "300\n",
      "(6257, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(word_to_index)\n",
    "embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "emb_matrix = np.zeros((vocab_len + 1, embed_vector_len))\n",
    "print(vocab_len)\n",
    "print(embed_vector_len)\n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in word_to_index.items():\n",
    "    embedding_vec = word_to_vec_map.get(word)\n",
    "    if embedding_vec is not None:\n",
    "        emb_matrix[index, :] = embedding_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_len + 1,\n",
    "    output_dim=embed_vector_len,\n",
    "    input_length=max_len,\n",
    "    weights=[emb_matrix],\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 300)          1877100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 150, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150, 128)          131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,360,045\n",
      "Trainable params: 482,945\n",
      "Non-trainable params: 1,877,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model((max_len,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 300)          1877100   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 148, 512)          461312    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 49, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 47, 256)           393472    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 13, 256)           196864    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,994,797\n",
      "Trainable params: 1,117,697\n",
      "Non-trainable params: 1,877,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convolution_model = conv_model((max_len,))\n",
    "convolution_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_indices = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(926, 150)\n"
     ]
    }
   ],
   "source": [
    "x_train_indices = pad_sequences(x_train_indices, maxlen=max_len, padding='post')\n",
    "print(x_train_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_indices = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_indices = pad_sequences(x_test_indices, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "convolution_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=10, verbose=2, mode='auto')\n",
    "checkpoint = ModelCheckpoint(filepath=\"best_weights_conv1d.hdf5\", verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 397 samples\n",
      "Epoch 1/15\n",
      "926/926 [==============================] - 10s 11ms/sample - loss: 0.5819 - accuracy: 0.7516 - val_loss: 0.5110 - val_accuracy: 0.8287\n",
      "Epoch 2/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.4992 - accuracy: 0.8121 - val_loss: 0.4889 - val_accuracy: 0.8287\n",
      "Epoch 3/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.4739 - accuracy: 0.8132 - val_loss: 0.4833 - val_accuracy: 0.8287\n",
      "Epoch 4/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.4473 - accuracy: 0.8121 - val_loss: 0.4792 - val_accuracy: 0.8287\n",
      "Epoch 5/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.4224 - accuracy: 0.8272 - val_loss: 0.4846 - val_accuracy: 0.8287\n",
      "Epoch 6/15\n",
      "926/926 [==============================] - 9s 9ms/sample - loss: 0.3931 - accuracy: 0.8283 - val_loss: 0.4798 - val_accuracy: 0.8287\n",
      "Epoch 7/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.3679 - accuracy: 0.8456 - val_loss: 0.4923 - val_accuracy: 0.8010\n",
      "Epoch 8/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.3378 - accuracy: 0.8575 - val_loss: 0.4751 - val_accuracy: 0.8287\n",
      "Epoch 9/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.2977 - accuracy: 0.8790 - val_loss: 0.4714 - val_accuracy: 0.8287\n",
      "Epoch 10/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.2516 - accuracy: 0.9082 - val_loss: 0.4727 - val_accuracy: 0.8287\n",
      "Epoch 11/15\n",
      "926/926 [==============================] - 10s 11ms/sample - loss: 0.2129 - accuracy: 0.9212 - val_loss: 0.4755 - val_accuracy: 0.8338\n",
      "Epoch 12/15\n",
      "926/926 [==============================] - 9s 9ms/sample - loss: 0.1757 - accuracy: 0.9341 - val_loss: 0.4995 - val_accuracy: 0.7683\n",
      "Epoch 13/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.1457 - accuracy: 0.9492 - val_loss: 0.5355 - val_accuracy: 0.8287\n",
      "Epoch 14/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.1148 - accuracy: 0.9687 - val_loss: 0.5545 - val_accuracy: 0.7280\n",
      "Epoch 15/15\n",
      "926/926 [==============================] - 8s 9ms/sample - loss: 0.0938 - accuracy: 0.9816 - val_loss: 0.5623 - val_accuracy: 0.8312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b4d315650>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv1D model\n",
    "convolution_model.fit(x_train_indices, y_train, batch_size=64, callbacks=[monitor, checkpoint], epochs=15, validation_data=(x_test_indices, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_model.load_weights('best_weights_conv1d.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926 samples, validate on 397 samples\n",
      "Epoch 1/15\n",
      "926/926 [==============================] - 38s 41ms/sample - loss: 0.6859 - accuracy: 0.8089 - val_loss: 0.6716 - val_accuracy: 0.8287\n",
      "Epoch 2/15\n",
      "926/926 [==============================] - 28s 31ms/sample - loss: 0.6556 - accuracy: 0.8110 - val_loss: 0.6182 - val_accuracy: 0.8287\n",
      "Epoch 3/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.5604 - accuracy: 0.8110 - val_loss: 0.4717 - val_accuracy: 0.8287\n",
      "Epoch 4/15\n",
      "926/926 [==============================] - 30s 32ms/sample - loss: 0.4941 - accuracy: 0.8121 - val_loss: 0.4634 - val_accuracy: 0.8287\n",
      "Epoch 5/15\n",
      "926/926 [==============================] - 30s 32ms/sample - loss: 0.4805 - accuracy: 0.8164 - val_loss: 0.4622 - val_accuracy: 0.8237\n",
      "Epoch 6/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4755 - accuracy: 0.8175 - val_loss: 0.4621 - val_accuracy: 0.8262\n",
      "Epoch 7/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4716 - accuracy: 0.8186 - val_loss: 0.4604 - val_accuracy: 0.8237\n",
      "Epoch 8/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4711 - accuracy: 0.8197 - val_loss: 0.4599 - val_accuracy: 0.8237\n",
      "Epoch 9/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4691 - accuracy: 0.8197 - val_loss: 0.4580 - val_accuracy: 0.8237\n",
      "Epoch 10/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4692 - accuracy: 0.8197 - val_loss: 0.4580 - val_accuracy: 0.8262\n",
      "Epoch 11/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4661 - accuracy: 0.8218 - val_loss: 0.4560 - val_accuracy: 0.8262\n",
      "Epoch 12/15\n",
      "926/926 [==============================] - 29s 32ms/sample - loss: 0.4653 - accuracy: 0.8218 - val_loss: 0.4562 - val_accuracy: 0.8262\n",
      "Epoch 13/15\n",
      "926/926 [==============================] - 29s 31ms/sample - loss: 0.4639 - accuracy: 0.8218 - val_loss: 0.4547 - val_accuracy: 0.8262\n",
      "Epoch 14/15\n",
      "926/926 [==============================] - 31s 34ms/sample - loss: 0.4653 - accuracy: 0.8229 - val_loss: 0.4519 - val_accuracy: 0.8312\n",
      "Epoch 15/15\n",
      "926/926 [==============================] - 33s 35ms/sample - loss: 0.4602 - accuracy: 0.8229 - val_loss: 0.4530 - val_accuracy: 0.8287\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=10, verbose=2, mode='auto')\n",
    "checkpoint = ModelCheckpoint(filepath=\"best_weights_lstm.hdf5\", verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit(x_train_indices, y_train, batch_size=64, callbacks=[monitor, checkpoint], epochs=15, validation_data=(x_test_indices, y_test))\n",
    "\n",
    "model.load_weights('best_weights_lstm.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_indices = tokenizer.texts_to_sequences(x_test)\n",
    "# x_test_indices = pad_sequences(x_test_indices, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "397/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 6s 14ms/sample - loss: 0.4410 - accuracy: 0.8312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4518824285764238, 0.8312343]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_indices, y_test)\n",
    "convolution_model.load_weights('best_weights_conv1d.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "397/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 4ms/sample - loss: 0.4609 - accuracy: 0.8287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4713668963620885, 0.8287154]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution_model.evaluate(x_test_indices, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'never win debate antivaxxer knowing good information makes worse throw shit declare victory'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.randint(0, len(x_test))\n",
    "x_test[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted sentiment is negative\n",
      "correct sentiment is positive\n"
     ]
    }
   ],
   "source": [
    "if predictions[n] > 0.5:\n",
    "    print('predicted sentiment is positive')\n",
    "else:\n",
    "    print('predicted sentiment is negative')\n",
    "    \n",
    "if y_test[n] == 1:\n",
    "    print('correct sentiment is positive')\n",
    "else:\n",
    "    print('correct sentiment is negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08736879]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(predictions[n])\n",
    "print(y_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_model.save_weights('best_weights_conv1d.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = tokenizer.texts_to_sequences(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_corpus\n",
    "data = predict_sentiments(data, corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['text', 'sentiment', 'sentiment score', 'predicted sentiment']].to_csv('clean_model_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>predicted sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>next time try not come things place bad faith ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.043835</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use youtube video try prove polio vaccine bad ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.133437</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>irrefutable prufe</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.307236</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bit crazy opinion would making illegal not was...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.108947</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sure know mercury vaccines doctors told millio...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.126902</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>mean not pic well son bitch going compliment s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.141321</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>answer yes yes</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.193283</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>girl work two strokes getting flue shot one el...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.140401</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>ok makes sense never chicken pox pretty sure g...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.181021</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>refuse flu vaccination fucking rethefuckfuse f...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1323 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     next time try not come things place bad faith ...  negative   \n",
       "1     use youtube video try prove polio vaccine bad ...  negative   \n",
       "2                                     irrefutable prufe  negative   \n",
       "3     bit crazy opinion would making illegal not was...  negative   \n",
       "4     sure know mercury vaccines doctors told millio...  negative   \n",
       "...                                                 ...       ...   \n",
       "1318  mean not pic well son bitch going compliment s...  negative   \n",
       "1319                                     answer yes yes  negative   \n",
       "1320  girl work two strokes getting flue shot one el...  negative   \n",
       "1321  ok makes sense never chicken pox pretty sure g...  negative   \n",
       "1322  refuse flu vaccination fucking rethefuckfuse f...  negative   \n",
       "\n",
       "      sentiment score predicted sentiment  \n",
       "0            0.043835            negative  \n",
       "1            0.133437            negative  \n",
       "2            0.307236            negative  \n",
       "3            0.108947            negative  \n",
       "4            0.126902            negative  \n",
       "...               ...                 ...  \n",
       "1318         0.141321            negative  \n",
       "1319         0.193283            negative  \n",
       "1320         0.140401            negative  \n",
       "1321         0.181021            negative  \n",
       "1322         0.190302            negative  \n",
       "\n",
       "[1323 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
